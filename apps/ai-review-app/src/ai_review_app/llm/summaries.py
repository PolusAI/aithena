_HARDCODED_SUMMARIES_ = [
"""
Summary of cluster 7
docs ids: 2210.16401v2, 2103.02893v2, 2304.10060v1, 1805.07880v1, 1602.02450v2, 1910.03231v7, 2106.05731v1, 2101.01366v2
doc titles:
`The Fisher-Rao Loss for Learning under Label Noise`
Lower-Bounded Proper Losses for Weakly Supervised Classification

Optimality of Robust Online Learning

Learning with Non-Convex Truncated Losses by SGD

Loss factorization, weakly supervised learning and label noise robustness

Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates

Leveraged Weighted Loss for Partial Label Learning

A Symmetric Loss Perspective of Reliable Machine Learning

Here is a summary of all documents, focusing on the general concepts they cover and discussing how they diverge or disagree:

Common themes:

Handling noisy labels: All documents deal with scenarios where training data contains incorrect or noisy labels.
Weak supervision: Many documents touch upon weakly supervised learning settings, where the model is trained with partial or corrupted labels.
Robust classification: The goal of these documents is to develop methods for robust classification in the presence of noise or corruption.
Divergent approaches:

Factorization vs. Peer Loss Functions: One document (Factorization) proposes a general framework for handling noisy labels using factorization, while another (Peer Loss Functions) introduces a new family of loss functions specifically designed to handle noisy labels.
Assumptions about noise rates: The documents differ in their assumptions about the availability and specification of noise rates. Some assume they are given (e.g., Peer Loss Functions), while others do not require them at all (e.g., Factorization).
Focus on specific problems: While some documents focus on general robust classification methods, others address specific problems like learning from partial labels (Leveraged Weighted Loss) or handling corrupted keywords and unlabeled documents in natural language processing (A Symmetric Loss Perspective of Reliable Machine Learning).
Agreement:

Importance of careful choice of loss functions: All documents stress the importance of choosing a suitable loss function when dealing with noisy labels.
Need for robust classification methods: The need for robust classification methods that can handle noise and corruption is a common theme across all documents.
Overall, while these documents share some common themes and goals, they diverge in their approaches, assumptions, and specific problems addressed.
"""
,
"""
Summary of cluster 5
docs ids: 2301.04956v2, 2002.07031v1, 2301.12184v2, 1501.06237v1, 1906.07658v2, 1611.00714v1, 1610.04351v1
doc titles:
`Graph Laplacian for Semi-Supervised Learning`
Regularizing Semi-supervised Graph Convolutional Networks with a Manifold Smoothness Loss

Laplacian-based Semi-Supervised Learning in Multilayer Hypergraphs by Coordinate Descent

Deep Transductive Semi-supervised Maximum Margin Clustering

Consistency of semi-supervised learning algorithms on graphs: Probit and one-hot methods

Scalable Semi-Supervised Learning over Networks using Nonsmooth Convex Optimization

Semi-supervised Graph Embedding Approach to Dynamic Link Prediction

User provided 12 articles.

The provided articles all relate to semi-supervised learning on graphs, with a focus on various methods and techniques for propagating labels from labeled data points to unlabeled ones. The common thread among these articles is their discussion of optimizing performance in graph-based classification problems.

Some general concepts that are covered across multiple articles include:

Using the graph Laplacian as a fundamental smoothing operator (articles 1, 3, and 7)
Employing spectral clustering methods (articles 2, 9, and 10)
Defining loss functions that balance supervised and unsupervised learning (articles 5, 6, and 11)
However, the articles diverge in their specific approaches to semi-supervised learning on graphs. For instance:

Some authors (e.g., articles 1, 3, and 7) propose new types of graph Laplacians adapted for SSL problems.
Others (e.g., articles 2, 9, and 10) focus on the use of spectral clustering methods to improve performance in SSL problems.
A few authors (e.g., articles 5, 6, and 11) emphasize the importance of consistency analysis in optimization-based techniques for graph-based classification.
In general, while all these articles address semi-supervised learning on graphs, they differ in their specific approaches, methodologies, and emphases.
""",

"""
Summary of cluster 0
docs ids: 2110.05876v2, 2002.11644v3, 2010.05469v1, 2009.04091v1, 2004.04674v1, 2107.01130v2, 2009.08796v1, 2006.04326v1, 2205.03169v1, 2011.02785v1, 2112.01642v1, 1905.10675v1, 1912.00385v4, 1611.00822v1
doc titles:
`Label-Aware Ranked Loss for robust People Counting using Automotive in-cabin Radar`
A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in Multi-output Classification Problems

CC-Loss: Channel Correlation Loss For Image Classification

Deep Metric Learning Meets Deep Clustering: An Novel Unsupervised Approach for Feature Embedding

Fisher Discriminant Triplet and Contrastive Losses for Training Siamese Networks

Ensemble of Loss Functions to Improve Generalizability of Deep Metric Learning methods

$Ïƒ^2$R Loss: a Weighted Loss by Multiplicative Factors using Sigmoidal Functions

Semi-Supervised Contrastive Learning with Generalized Contrastive Loss and Its Application to Speaker Recognition

The NT-Xent loss upper bound

Deep Metric Learning with Spherical Embedding

Probabilistic Contrastive Loss for Self-Supervised Learning

Constellation Loss: Improving the efficiency of deep metric learning loss functions for optimal embedding

The Group Loss for Deep Metric Learning

Learning Deep Embeddings with Histogram Loss

The documents provided all relate to deep metric learning, a technique used in artificial intelligence and machine learning. The general concepts covered include:

Contrastive Learning: Most documents discuss variations of contrastive loss functions, which are designed to learn discriminative feature embeddings that can be used for tasks such as clustering, image retrieval, and classification.
Embedding Similarity: A common goal among these methods is to encourage similar samples (e.g., images of the same class) to have similar embeddings while promoting dissimilarity between samples from different classes.
Loss Function Design: The documents propose new or modify existing loss functions to achieve better performance in deep metric learning tasks. These losses often involve differentiable label-propagation methods, soft assignment operations, and histogram-based calculations.
However, the documents diverge or disagree on specific aspects such as:

Specific Loss Functions: Different authors propose distinct loss functions (e.g., Contrastive Loss, Triplet Loss, Multi-Class N-Pair Loss, Constellation Loss, Group Loss) tailored to their respective methodologies.
Assumptions and Properties: Some documents highlight specific properties of the proposed losses (e.g., piecewise-differentiability, the use of histograms), while others focus on theoretical or empirical justifications for their methods.
Applications and Performance: While most documents demonstrate state-of-the-art results on various tasks like clustering and image retrieval, they may differ in their emphasis on specific applications or datasets.
In summary, these documents collectively contribute to the advancement of deep metric learning by introducing novel loss functions, exploring differentiable label-propagation methods, and demonstrating improved performance on various tasks. However, each document has its unique focus, methodology, or application, highlighting the diversity within this research area.
"""
]